{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "e8d50f63",
            "metadata": {},
            "source": [
                "# Interactive Heart Segmentation Visualization\n",
                "\n",
                "This notebook allows you to visualize model predictions with increasing detail:\n",
                "1.  **Validation Set**: Standard comparison (GT vs Pred).\n",
                "2.  **Test Set**: Inference on unseen data.\n",
                "3.  **Extended Visualization**: Heatmaps (Probabilities) and Contour analysis."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "b6f96833",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import torch\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from glob import glob\n",
                "from monai.networks.nets import UNet\n",
                "from monai.networks.layers import Norm\n",
                "from monai.transforms import (\n",
                "    Compose, LoadImaged, EnsureChannelFirstd, NormalizeIntensityd,\n",
                "    Orientationd, Spacingd, ToTensord, DivisiblePadd, CropForegroundd\n",
                ")\n",
                "from pathlib import Path\n",
                "from ipywidgets import interact, IntSlider"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7ab6fa9a",
            "metadata": {},
            "source": [
                "## 1. Setup & Load Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "95390e26",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model path: c:\\Users\\swapn\\code\\AI Healthcare Imaging\\results\\best_metric_model.pth\n",
                        "Using device: cpu\n",
                        "Model weights loaded successfully.\n"
                    ]
                }
            ],
            "source": [
                "# Determine paths\n",
                "BASE_DIR = Path(os.getcwd())\n",
                "DATASET_DIR = BASE_DIR / \"datasets\"\n",
                "RESULTS_DIR = BASE_DIR / \"results\"\n",
                "MODEL_PATH = RESULTS_DIR / \"best_metric_model.pth\"\n",
                "\n",
                "print(f\"Model path: {MODEL_PATH}\")\n",
                "\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")\n",
                "\n",
                "# Initialize Model\n",
                "model = UNet(\n",
                "    spatial_dims=3,\n",
                "    in_channels=1,\n",
                "    out_channels=2,\n",
                "    channels=(16, 32, 64, 128, 256),\n",
                "    strides=(2, 2, 2, 2),\n",
                "    num_res_units=2,\n",
                "    norm=Norm.BATCH,\n",
                "    dropout=0.2,\n",
                ").to(device)\n",
                "\n",
                "# Load Weights\n",
                "if MODEL_PATH.exists():\n",
                "    state_dict = torch.load(MODEL_PATH, map_location=device)\n",
                "    if 'model_state_dict' in state_dict:\n",
                "        model.load_state_dict(state_dict['model_state_dict'])\n",
                "    else:\n",
                "        model.load_state_dict(state_dict)\n",
                "    print(\"Model weights loaded successfully.\")\n",
                "    model.eval()\n",
                "else:\n",
                "    print(\"WARNING: Model file not found!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "34eb123a",
            "metadata": {},
            "source": [
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f52af094",
            "metadata": {},
            "source": [
                "## 2. Part A: Validation Set (With Ground Truth)\n",
                "Standard view: Comparing Ground Truth vs Prediction masks."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "a27c7426",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Validation samples: 4\n"
                    ]
                }
            ],
            "source": [
                "PIXDIM = (1.5, 1.5, 1.0)\n",
                "\n",
                "# Transforms for Validation (Includes Label)\n",
                "val_transforms = Compose([\n",
                "    LoadImaged(keys=[\"image\", \"label\"]),\n",
                "    EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
                "    Spacingd(keys=[\"image\", \"label\"], pixdim=PIXDIM, mode=(\"bilinear\", \"nearest\")),\n",
                "    Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\", labels=None),\n",
                "    NormalizeIntensityd(keys=[\"image\"], nonzero=True, channel_wise=True),\n",
                "    CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
                "    DivisiblePadd(keys=[\"image\", \"label\"], k=16),\n",
                "    ToTensord(keys=[\"image\", \"label\"]),\n",
                "])\n",
                "\n",
                "# Load Validation Files (Last 20% of training data)\n",
                "images_tr = sorted([str(p) for p in (DATASET_DIR / \"imagesTr\").glob(\"*.nii\")])\n",
                "labels_tr = sorted([str(p) for p in (DATASET_DIR / \"labelsTr\").glob(\"*.nii\")])\n",
                "\n",
                "data_tr = [{\"image\": i, \"label\": l} for i, l in zip(images_tr, labels_tr)]\n",
                "val_files = data_tr[int(len(data_tr)*0.8):]\n",
                "\n",
                "print(f\"Validation samples: {len(val_files)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "420c7124",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Visualizing Validation Set (Images with Labels)...\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "8ae02e438b284b69bef859595bf4dcc8",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "interactive(children=(IntSlider(value=0, description='sample_idx', max=3), Output()), _dom_classes=('widget-in…"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/plain": [
                            "<function __main__.visualize_validation(sample_idx=0)>"
                        ]
                    },
                    "execution_count": 9,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "def visualize_validation(sample_idx=0):\n",
                "    if sample_idx >= len(val_files):\n",
                "        return\n",
                "\n",
                "    # print(f\"Loading Validation Sample {sample_idx}: {Path(val_files[sample_idx]['image']).name}\")\n",
                "    data = val_transforms(val_files[sample_idx])\n",
                "    image = data[\"image\"].unsqueeze(0).to(device)\n",
                "    label = data[\"label\"].unsqueeze(0).to(device)\n",
                "\n",
                "    model.eval()\n",
                "    with torch.no_grad():\n",
                "        pred = (torch.sigmoid(model(image)) > 0.5).float()\n",
                "\n",
                "    img_vol = image[0, 0].cpu().numpy()\n",
                "    lbl_vol = label[0, 0].cpu().numpy()\n",
                "    pred_vol = pred[0, 1].cpu().numpy()\n",
                "\n",
                "    def plot(slice_idx):\n",
                "        fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
                "        # 1. MRI\n",
                "        axes[0].imshow(img_vol[:, :, slice_idx], cmap='gray')\n",
                "        axes[0].set_title(f\"MRI Slice {slice_idx}\")\n",
                "        axes[0].axis('off')\n",
                "        # 2. GT\n",
                "        axes[1].imshow(lbl_vol[:, :, slice_idx], cmap='gray')\n",
                "        axes[1].set_title(\"Ground Truth\")\n",
                "        axes[1].axis('off')\n",
                "        # 3. Pred\n",
                "        axes[2].imshow(pred_vol[:, :, slice_idx], cmap='gray')\n",
                "        axes[2].set_title(\"Prediction\")\n",
                "        axes[2].axis('off')\n",
                "        # 4. Overlay\n",
                "        axes[3].imshow(img_vol[:, :, slice_idx], cmap='gray')\n",
                "        overlay = np.zeros((*img_vol.shape[:2], 4))\n",
                "        overlay[lbl_vol[:, :, slice_idx] > 0, 1] = 1.0 # Green=GT\n",
                "        overlay[lbl_vol[:, :, slice_idx] > 0, 3] = 0.3\n",
                "        overlay[pred_vol[:, :, slice_idx] > 0, 0] = 1.0 # Red=Pred\n",
                "        overlay[pred_vol[:, :, slice_idx] > 0, 3] = 0.5\n",
                "        axes[3].imshow(overlay)\n",
                "        axes[3].set_title(\"Overlay (G=GT, R=Pred)\")\n",
                "        axes[3].axis('off')\n",
                "        plt.show()\n",
                "\n",
                "    interact(plot, slice_idx=IntSlider(min=0, max=img_vol.shape[2]-1, step=1, value=img_vol.shape[2]//2))\n",
                "\n",
                "print(\"Visualizing Validation Set (Images with Labels)...\")\n",
                "interact(visualize_validation, sample_idx=IntSlider(min=0, max=len(val_files)-1, step=1, value=0))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2d676131",
            "metadata": {},
            "source": [
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ffdfe055",
            "metadata": {},
            "source": [
                "## 3. Part B: Test Set (Inference Only)\n",
                "For unseen images with no labels."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "27ed4c03",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Test samples: 10\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\swapn\\code\\AI Healthcare Imaging\\.venv\\Lib\\site-packages\\monai\\utils\\deprecate_utils.py:321: FutureWarning: monai.transforms.spatial.dictionary Orientationd.__init__:labels: Current default value of argument `labels=(('L', 'R'), ('P', 'A'), ('I', 'S'))` was changed in version None from `labels=(('L', 'R'), ('P', 'A'), ('I', 'S'))` to `labels=None`. Default value changed to None meaning that the transform now uses the 'space' of a meta-tensor, if applicable, to determine appropriate axis labels.\n",
                        "  warn_deprecated(argname, msg, warning_category)\n"
                    ]
                }
            ],
            "source": [
                "# Transforms for Test (Image Only)\n",
                "test_transforms = Compose([\n",
                "    LoadImaged(keys=[\"image\"]),\n",
                "    EnsureChannelFirstd(keys=[\"image\"]),\n",
                "    Spacingd(keys=[\"image\"], pixdim=PIXDIM, mode=(\"bilinear\")),\n",
                "    Orientationd(keys=[\"image\"], axcodes=\"RAS\"),\n",
                "    NormalizeIntensityd(keys=[\"image\"], nonzero=True, channel_wise=True),\n",
                "    CropForegroundd(keys=[\"image\"], source_key=\"image\"),\n",
                "    DivisiblePadd(keys=[\"image\"], k=16),\n",
                "    ToTensord(keys=[\"image\"]),\n",
                "])\n",
                "\n",
                "# Load Test Files\n",
                "images_ts = sorted([str(p) for p in (DATASET_DIR / \"imagesTs\").glob(\"*.nii\")])\n",
                "test_files = [{\"image\": i} for i in images_ts]\n",
                "\n",
                "print(f\"Test samples: {len(test_files)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "77a86145",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Visualizing Test Set (Unseen, No Labels)...\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "bf5afff21e2445d3a188e797feeff1e8",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "interactive(children=(IntSlider(value=0, description='sample_idx', max=9), Output()), _dom_classes=('widget-in…"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/plain": [
                            "<function __main__.visualize_test(sample_idx=0)>"
                        ]
                    },
                    "execution_count": 11,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "def visualize_test(sample_idx=0):\n",
                "    if sample_idx >= len(test_files):\n",
                "        return\n",
                "\n",
                "    # print(f\"Loading Test Sample {sample_idx}: {Path(test_files[sample_idx]['image']).name}\")\n",
                "    data = test_transforms(test_files[sample_idx])\n",
                "    image = data[\"image\"].unsqueeze(0).to(device)\n",
                "\n",
                "    model.eval()\n",
                "    with torch.no_grad():\n",
                "        pred = (torch.sigmoid(model(image)) > 0.5).float()\n",
                "\n",
                "    img_vol = image[0, 0].cpu().numpy()\n",
                "    pred_vol = pred[0, 1].cpu().numpy()\n",
                "\n",
                "    def plot(slice_idx):\n",
                "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
                "        axes[0].imshow(img_vol[:, :, slice_idx], cmap='gray')\n",
                "        axes[0].set_title(f\"MRI Slice {slice_idx}\")\n",
                "        axes[0].axis('off')\n",
                "        \n",
                "        axes[1].imshow(pred_vol[:, :, slice_idx], cmap='gray')\n",
                "        axes[1].set_title(\"Predicted Mask\")\n",
                "        axes[1].axis('off')\n",
                "        \n",
                "        axes[2].imshow(img_vol[:, :, slice_idx], cmap='gray')\n",
                "        overlay = np.zeros((*img_vol.shape[:2], 4))\n",
                "        overlay[pred_vol[:, :, slice_idx] > 0, 0] = 1.0 # Red=Pred\n",
                "        overlay[pred_vol[:, :, slice_idx] > 0, 3] = 0.5\n",
                "        axes[2].imshow(overlay)\n",
                "        axes[2].set_title(\"Prediction Overlay\")\n",
                "        axes[2].axis('off')\n",
                "        plt.show()\n",
                "\n",
                "    interact(plot, slice_idx=IntSlider(min=0, max=img_vol.shape[2]-1, step=1, value=img_vol.shape[2]//2))\n",
                "\n",
                "print(\"Visualizing Test Set (Unseen, No Labels)...\")\n",
                "interact(visualize_test, sample_idx=IntSlider(min=0, max=len(test_files)-1, step=1, value=0))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "335c4586",
            "metadata": {},
            "source": [
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8f36e864",
            "metadata": {},
            "source": [
                "## 4. Part C: Extended Visualization\n",
                "**Advanced Analysis**: Heatmaps (Probabilities) and Contours.\n",
                "This helps identify \"uncertain\" areas where the model is confident but not perfectly binary."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "4bb184d4",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Extended Visualization (Heatmaps & Contours)...\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "6bd7785c007249d99c33dddbf66ab798",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "interactive(children=(IntSlider(value=0, description='sample_idx', max=3), Output()), _dom_classes=('widget-in…"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/plain": [
                            "<function __main__.visualize_extended(sample_idx=0)>"
                        ]
                    },
                    "execution_count": 12,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "def visualize_extended(sample_idx=0):\n",
                "    if sample_idx >= len(val_files):\n",
                "        return\n",
                "\n",
                "    # print(f\"Loading Sample {sample_idx} for Extended Viz...\")\n",
                "    data = val_transforms(val_files[sample_idx])\n",
                "    image = data[\"image\"].unsqueeze(0).to(device)\n",
                "    label = data[\"label\"].unsqueeze(0).to(device)\n",
                "\n",
                "    model.eval()\n",
                "    with torch.no_grad():\n",
                "        logits = model(image)\n",
                "        probs = torch.sigmoid(logits)\n",
                "        pred = (probs > 0.5).float()\n",
                "\n",
                "    img_vol = image[0, 0].cpu().numpy()\n",
                "    lbl_vol = label[0, 0].cpu().numpy()\n",
                "    prob_vol = probs[0, 1].cpu().numpy() # Probability map\n",
                "    \n",
                "    def plot(slice_idx):\n",
                "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
                "        \n",
                "        # 1. Probability Heatmap\n",
                "        # Shows model confidence (0=Black, 1=Hot/White)\n",
                "        im1 = axes[0].imshow(prob_vol[:, :, slice_idx], cmap='inferno', vmin=0, vmax=1)\n",
                "        axes[0].set_title(\"Probability Heatmap (Model Confidence)\")\n",
                "        axes[0].axis('off')\n",
                "        plt.colorbar(im1, ax=axes[0], fraction=0.046, pad=0.04)\n",
                "        \n",
                "        # 2. Contour Plot\n",
                "        # Overlay Lines: Green=GT, Red=Pred\n",
                "        axes[1].imshow(img_vol[:, :, slice_idx], cmap='gray')\n",
                "        # Ground Truth Contour\n",
                "        if np.any(lbl_vol[:, :, slice_idx]):\n",
                "            axes[1].contour(lbl_vol[:, :, slice_idx], levels=[0.5], colors='lime', linewidths=2, label='GT')\n",
                "        # Prediction Contour\n",
                "        if np.any(pred[:, 0, :, :, slice_idx].cpu().numpy()):\n",
                "             axes[1].contour(pred[:, 0, :, :, slice_idx].cpu().numpy(), levels=[0.5], colors='red', linewidths=2, linestyles='dashed', label='Pred')\n",
                "        axes[1].set_title(\"Contour Overlay (Green=GT, Red=Pred)\")\n",
                "        axes[1].axis('off')\n",
                "\n",
                "        # 3. Enhanced Overlay (Hybrid)\n",
                "        axes[2].imshow(img_vol[:, :, slice_idx], cmap='gray')\n",
                "        axes[2].imshow(prob_vol[:, :, slice_idx], cmap='jet', alpha=0.3) # Weak overlay of probability\n",
                "        if np.any(lbl_vol[:, :, slice_idx]):\n",
                "            axes[2].contour(lbl_vol[:, :, slice_idx], levels=[0.5], colors='white', linewidths=1.5)\n",
                "        axes[2].set_title(\"Hybrid (MRI + Heatmap + GT Contour)\")\n",
                "        axes[2].axis('off')\n",
                "\n",
                "        plt.show()\n",
                "\n",
                "    interact(plot, slice_idx=IntSlider(min=0, max=img_vol.shape[2]-1, step=1, value=img_vol.shape[2]//2))\n",
                "\n",
                "print(\"Extended Visualization (Heatmaps & Contours)...\")\n",
                "interact(visualize_extended, sample_idx=IntSlider(min=0, max=len(val_files)-1, step=1, value=0))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7f95f632",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
