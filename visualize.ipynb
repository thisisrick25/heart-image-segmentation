{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Heart Segmentation Visualization\n",
    "\n",
    "This notebook allows you to visualize model predictions with increasing detail:\n",
    "1.  **Validation Set**: Standard comparison (GT vs Pred).\n",
    "2.  **Test Set**: Inference on unseen data.\n",
    "3.  **Extended Visualization**: Heatmaps (Probabilities) and Contour analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5c2c209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from monai.networks.nets import UNet\n",
    "from monai.networks.layers import Norm\n",
    "from monai.transforms import (\n",
    "    Compose, LoadImaged, EnsureChannelFirstd, NormalizeIntensityd,\n",
    "    Orientationd, Spacingd, ToTensord, DivisiblePadd, CropForegroundd\n",
    ")\n",
    "from pathlib import Path\n",
    "from ipywidgets import interact, IntSlider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c819fac",
   "metadata": {},
   "source": [
    "## 1. Setup & Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71a04113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model path: c:\\Users\\swapn\\code\\AI Healthcare Imaging\\results\\best_metric_model.pth\n",
      "Using device: cpu\n",
      "Model weights loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Determine paths\n",
    "BASE_DIR = Path(os.getcwd())\n",
    "DATASET_DIR = BASE_DIR / \"datasets\"\n",
    "RESULTS_DIR = BASE_DIR / \"results\"\n",
    "MODEL_PATH = RESULTS_DIR / \"best_metric_model.pth\"\n",
    "\n",
    "print(f\"Model path: {MODEL_PATH}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize Model\n",
    "model = UNet(\n",
    "    spatial_dims=3,\n",
    "    in_channels=1,\n",
    "    out_channels=2,\n",
    "    channels=(16, 32, 64, 128, 256),\n",
    "    strides=(2, 2, 2, 2),\n",
    "    num_res_units=2,\n",
    "    norm=Norm.BATCH,\n",
    "    dropout=0.2,\n",
    ").to(device)\n",
    "\n",
    "# Load Weights\n",
    "if MODEL_PATH.exists():\n",
    "    state_dict = torch.load(MODEL_PATH, map_location=device)\n",
    "    if 'model_state_dict' in state_dict:\n",
    "        model.load_state_dict(state_dict['model_state_dict'])\n",
    "    else:\n",
    "        model.load_state_dict(state_dict)\n",
    "    print(\"Model weights loaded successfully.\")\n",
    "    model.eval()\n",
    "else:\n",
    "    print(\"WARNING: Model file not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59202815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dice_score(pred, gt):\n",
    "    \"\"\"\n",
    "    Calculate Dice score between prediction and ground truth.\n",
    "    \n",
    "    Args:\n",
    "        pred: Binary prediction volume (numpy array)\n",
    "        gt: Binary ground truth volume (numpy array)\n",
    "    \n",
    "    Returns:\n",
    "        float: Dice score (0-1)\n",
    "    \"\"\"\n",
    "    intersection = np.sum(pred * gt)\n",
    "    union = np.sum(pred) + np.sum(gt)\n",
    "    \n",
    "    if union == 0:\n",
    "        return 1.0 if np.sum(pred) == 0 and np.sum(gt) == 0 else 0.0\n",
    "    \n",
    "    return 2.0 * intersection / union\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab273d9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdbf1f4",
   "metadata": {},
   "source": [
    "## 2. Part A: Validation Set (With Ground Truth)\n",
    "Standard view: Comparing Ground Truth vs Prediction masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d18100ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation samples: 4\n"
     ]
    }
   ],
   "source": [
    "PIXDIM = (1.5, 1.5, 1.0)\n",
    "\n",
    "# Transforms for Validation (Includes Label)\n",
    "val_transforms = Compose([\n",
    "    LoadImaged(keys=[\"image\", \"label\"]),\n",
    "    EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "    Spacingd(keys=[\"image\", \"label\"], pixdim=PIXDIM, mode=(\"bilinear\", \"nearest\")),\n",
    "    Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\", labels=None),\n",
    "    NormalizeIntensityd(keys=[\"image\"], nonzero=True, channel_wise=True),\n",
    "    CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "    DivisiblePadd(keys=[\"image\", \"label\"], k=16),\n",
    "    ToTensord(keys=[\"image\", \"label\"]),\n",
    "])\n",
    "\n",
    "# Load Validation Files (Last 20% of training data)\n",
    "images_tr = sorted([str(p) for p in (DATASET_DIR / \"imagesTr\").glob(\"*.nii\")])\n",
    "labels_tr = sorted([str(p) for p in (DATASET_DIR / \"labelsTr\").glob(\"*.nii\")])\n",
    "\n",
    "data_tr = [{\"image\": i, \"label\": l} for i, l in zip(images_tr, labels_tr)]\n",
    "val_files = data_tr[int(len(data_tr)*0.8):]\n",
    "\n",
    "print(f\"Validation samples: {len(val_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b3e7c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing Validation Set (Images with Labels)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed9f05575d784463ac50ba266bd4cedc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='sample_idx', max=3), Output()), _dom_classes=('widget-in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.visualize_validation(sample_idx=0)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def visualize_validation(sample_idx=0):\n",
    "    if sample_idx >= len(val_files):\n",
    "        return\n",
    "\n",
    "    # print(f\"Loading Validation Sample {sample_idx}: {Path(val_files[sample_idx]['image']).name}\")\n",
    "    data = val_transforms(val_files[sample_idx])\n",
    "    image = data[\"image\"].unsqueeze(0).to(device)\n",
    "    label = data[\"label\"].unsqueeze(0).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = (torch.sigmoid(model(image)) > 0.5).float()\n",
    "\n",
    "    img_vol = image[0, 0].cpu().numpy()\n",
    "    lbl_vol = label[0, 0].cpu().numpy()\n",
    "    pred_vol = pred[0, 1].cpu().numpy()\n",
    "    \n",
    "    # Calculate Dice score\n",
    "    dice_score = calculate_dice_score(pred_vol, lbl_vol)\n",
    "\n",
    "    def plot(slice_idx):\n",
    "        fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "        # 1. MRI\n",
    "        axes[0].imshow(img_vol[:, :, slice_idx], cmap='gray')\n",
    "        axes[0].set_title(f\"MRI Slice {slice_idx}\")\n",
    "        axes[0].axis('off')\n",
    "        # 2. GT\n",
    "        axes[1].imshow(lbl_vol[:, :, slice_idx], cmap='gray')\n",
    "        axes[1].set_title(\"Ground Truth\")\n",
    "        axes[1].axis('off')\n",
    "        # 3. Pred\n",
    "        axes[2].imshow(pred_vol[:, :, slice_idx], cmap='gray')\n",
    "        axes[2].set_title(f\"Prediction (Dice: {dice_score:.4f})\")\n",
    "        axes[2].axis('off')\n",
    "        # 4. Overlay\n",
    "        axes[3].imshow(img_vol[:, :, slice_idx], cmap='gray')\n",
    "        overlay = np.zeros((*img_vol.shape[:2], 4))\n",
    "        \n",
    "        # GT in green (0, 1, 0)\n",
    "        gt_mask = lbl_vol[:, :, slice_idx] > 0\n",
    "        overlay[gt_mask, 1] = 1.0  # Green channel\n",
    "        overlay[gt_mask, 3] = 0.4  # Alpha\n",
    "        \n",
    "        # Pred in red (1, 0, 0)\n",
    "        pred_mask = pred_vol[:, :, slice_idx] > 0\n",
    "        overlay[pred_mask, 0] = 1.0  # Red channel\n",
    "        overlay[pred_mask, 3] = 0.5  # Alpha\n",
    "        \n",
    "        # Where they overlap, show yellow (1, 1, 0)\n",
    "        overlap_mask = gt_mask & pred_mask\n",
    "        overlay[overlap_mask, 0] = 1.0  # Red\n",
    "        overlay[overlap_mask, 1] = 1.0  # Green\n",
    "        overlay[overlap_mask, 3] = 0.6  # Stronger alpha for overlap\n",
    "        \n",
    "        axes[3].imshow(overlay)\n",
    "        axes[3].set_title(\"Overlay (Green=GT, Red=Pred, Yellow=Both)\")\n",
    "        axes[3].axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    interact(plot, slice_idx=IntSlider(min=0, max=img_vol.shape[2]-1, step=1, value=img_vol.shape[2]//2))\n",
    "\n",
    "print(\"Visualizing Validation Set (Images with Labels)...\")\n",
    "interact(visualize_validation, sample_idx=IntSlider(min=0, max=len(val_files)-1, step=1, value=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd6dc7e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac28081",
   "metadata": {},
   "source": [
    "## 3. Part B: Test Set (Inference Only)\n",
    "For unseen images with no labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c24b0d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test samples: 10\n"
     ]
    }
   ],
   "source": [
    "# Transforms for Test (Image Only)\n",
    "test_transforms = Compose([\n",
    "    LoadImaged(keys=[\"image\"]),\n",
    "    EnsureChannelFirstd(keys=[\"image\"]),\n",
    "    Spacingd(keys=[\"image\"], pixdim=PIXDIM, mode=(\"bilinear\")),\n",
    "    Orientationd(keys=[\"image\"], axcodes=\"RAS\"),\n",
    "    NormalizeIntensityd(keys=[\"image\"], nonzero=True, channel_wise=True),\n",
    "    CropForegroundd(keys=[\"image\"], source_key=\"image\"),\n",
    "    DivisiblePadd(keys=[\"image\"], k=16),\n",
    "    ToTensord(keys=[\"image\"]),\n",
    "])\n",
    "\n",
    "# Load Test Files\n",
    "images_ts = sorted([str(p) for p in (DATASET_DIR / \"imagesTs\").glob(\"*.nii\")])\n",
    "test_files = [{\"image\": i} for i in images_ts]\n",
    "\n",
    "print(f\"Test samples: {len(test_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "024dd101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing Test Set (Unseen, No Labels)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5d4a2e0a5594c2894c541de52af1e82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='sample_idx', max=9), Output()), _dom_classes=('widget-in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.visualize_test(sample_idx=0)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def visualize_test(sample_idx=0):\n",
    "    if sample_idx >= len(test_files):\n",
    "        return\n",
    "\n",
    "    # print(f\"Loading Test Sample {sample_idx}: {Path(test_files[sample_idx]['image']).name}\")\n",
    "    data = test_transforms(test_files[sample_idx])\n",
    "    image = data[\"image\"].unsqueeze(0).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(image)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        pred = (probs > 0.5).float()\n",
    "\n",
    "    img_vol = image[0, 0].cpu().numpy()\n",
    "    pred_vol = pred[0, 1].cpu().numpy()\n",
    "    prob_vol = probs[0, 1].cpu().numpy()\n",
    "    \n",
    "    # Calculate prediction statistics\n",
    "    pred_voxels = np.sum(pred_vol > 0)\n",
    "    total_voxels = pred_vol.size\n",
    "    pred_percentage = 100.0 * pred_voxels / total_voxels\n",
    "    mean_confidence = np.mean(prob_vol[pred_vol > 0]) if pred_voxels > 0 else 0.0\n",
    "\n",
    "    def plot(slice_idx):\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        axes[0].imshow(img_vol[:, :, slice_idx], cmap='gray')\n",
    "        axes[0].set_title(f\"MRI Slice {slice_idx}\")\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        axes[1].imshow(pred_vol[:, :, slice_idx], cmap='gray')\n",
    "        axes[1].set_title(f\"Predicted Mask\\n(Conf: {mean_confidence:.3f}, Vol: {pred_percentage:.2f}%)\")\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        axes[2].imshow(img_vol[:, :, slice_idx], cmap='gray')\n",
    "        overlay = np.zeros((*img_vol.shape[:2], 4))\n",
    "        overlay[pred_vol[:, :, slice_idx] > 0, 0] = 1.0 # Red=Pred\n",
    "        overlay[pred_vol[:, :, slice_idx] > 0, 3] = 0.5\n",
    "        axes[2].imshow(overlay)\n",
    "        axes[2].set_title(\"Prediction Overlay\")\n",
    "        axes[2].axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    interact(plot, slice_idx=IntSlider(min=0, max=img_vol.shape[2]-1, step=1, value=img_vol.shape[2]//2))\n",
    "\n",
    "print(\"Visualizing Test Set (Unseen, No Labels)...\")\n",
    "interact(visualize_test, sample_idx=IntSlider(min=0, max=len(test_files)-1, step=1, value=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dfadde",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3eda34",
   "metadata": {},
   "source": [
    "## 4. Part C: Extended Visualization\n",
    "**Advanced Analysis**: Heatmaps (Probabilities) and Contours.\n",
    "This helps identify \"uncertain\" areas where the model is confident but not perfectly binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69534c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended Visualization (Heatmaps & Contours)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82695dc00a004b8b8804975a94c409dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='sample_idx', max=3), Output()), _dom_classes=('widget-in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.visualize_extended(sample_idx=0)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def visualize_extended(sample_idx=0):\n",
    "    if sample_idx >= len(val_files):\n",
    "        return\n",
    "\n",
    "    # print(f\"Loading Sample {sample_idx} for Extended Viz...\")\n",
    "    data = val_transforms(val_files[sample_idx])\n",
    "    image = data[\"image\"].unsqueeze(0).to(device)\n",
    "    label = data[\"label\"].unsqueeze(0).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(image)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        pred = (probs > 0.5).float()\n",
    "\n",
    "    img_vol = image[0, 0].cpu().numpy()\n",
    "    lbl_vol = label[0, 0].cpu().numpy()\n",
    "    prob_vol = probs[0, 1].cpu().numpy() # Probability map\n",
    "    pred_vol = pred[0, 1].cpu().numpy() # Prediction volume\n",
    "    \n",
    "    # Calculate Dice score\n",
    "    dice_score = calculate_dice_score(pred_vol, lbl_vol)\n",
    "    \n",
    "    # Calculate prediction statistics\n",
    "    pred_voxels = np.sum(pred_vol > 0)\n",
    "    total_voxels = pred_vol.size\n",
    "    pred_percentage = 100.0 * pred_voxels / total_voxels\n",
    "    mean_confidence = np.mean(prob_vol[pred_vol > 0]) if pred_voxels > 0 else 0.0\n",
    "    \n",
    "    def plot(slice_idx):\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "        \n",
    "        # 1. Probability Heatmap\n",
    "        # Shows model confidence (0=Black, 1=Hot/White)\n",
    "        im1 = axes[0].imshow(prob_vol[:, :, slice_idx], cmap='inferno', vmin=0, vmax=1)\n",
    "        axes[0].set_title(f\"Probability Heatmap\\n(Dice: {dice_score:.4f}, Conf: {mean_confidence:.3f})\")\n",
    "        axes[0].axis('off')\n",
    "        plt.colorbar(im1, ax=axes[0], fraction=0.046, pad=0.04)\n",
    "        \n",
    "        # 2. Contour Plot\n",
    "        # Overlay Lines: Green=GT, Red=Pred\n",
    "        axes[1].imshow(img_vol[:, :, slice_idx], cmap='gray')\n",
    "        # Ground Truth Contour\n",
    "        if np.any(lbl_vol[:, :, slice_idx]):\n",
    "            axes[1].contour(lbl_vol[:, :, slice_idx], levels=[0.5], colors='lime', linewidths=2)\n",
    "        # Prediction Contour\n",
    "        if np.any(pred_vol[:, :, slice_idx]):\n",
    "            axes[1].contour(pred_vol[:, :, slice_idx], levels=[0.5], colors='red', linewidths=2, linestyles='dashed')\n",
    "        axes[1].set_title(f\"Contour Overlay\\n(Vol: {pred_percentage:.2f}%)\")\n",
    "        axes[1].axis('off')\n",
    "\n",
    "        # 3. Enhanced Overlay (Hybrid)\n",
    "        axes[2].imshow(img_vol[:, :, slice_idx], cmap='gray')\n",
    "        \n",
    "        # Mask low probabilities to make background transparent\n",
    "        # This prevents the whole image from being tinted blue (jet's 0 value)\n",
    "        masked_prob = np.ma.masked_where(prob_vol[:, :, slice_idx] < 0.1, prob_vol[:, :, slice_idx])\n",
    "        \n",
    "        axes[2].imshow(masked_prob, cmap='jet', alpha=0.6, vmin=0, vmax=1) # Stronger overlay on ROI only\n",
    "        if np.any(lbl_vol[:, :, slice_idx]):\n",
    "            axes[2].contour(lbl_vol[:, :, slice_idx], levels=[0.5], colors='white', linewidths=1.5)\n",
    "        axes[2].set_title(\"Hybrid (MRI + Heatmap + GT Contour)\")\n",
    "        axes[2].axis('off')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    interact(plot, slice_idx=IntSlider(min=0, max=img_vol.shape[2]-1, step=1, value=img_vol.shape[2]//2))\n",
    "\n",
    "print(\"Extended Visualization (Heatmaps & Contours)...\")\n",
    "interact(visualize_extended, sample_idx=IntSlider(min=0, max=len(val_files)-1, step=1, value=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7ffe2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
